{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始数据的处理:\n",
    "1.将文本转换为Tokens\n",
    "2.删除低频词语，添加UNKNOWN_TOKEN\n",
    "3.添加开始和结束标记\n",
    "4.建立训练数据矩阵(单词到序号的映射)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65408 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'documentary' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print(\"Reading CSV file...\")\n",
    "with open('data/reddit-comments-2015-08.csv', 'rt') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    # 跳过第一个body\n",
    "    next(reader)\n",
    "    # 将剩下的所有评论都按照句子划分，并且都小写\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # 增加SENTENCE_START和SENTENCE_END标记\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print (\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print (\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print (\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training data\n",
    "# x,y分别为输入和目标输出，其中，x为前面n-1个单词，y为后面n-1个单词，并且保存为词汇表的序号\n",
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 858, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 858, 54, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = x_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\"%(\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\"%(\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的输入x将会是一串单词，但是，我们需要将上面的序号转换为one-hot编码方式，其中，公式为:\n",
    "s(t)=tanh(U*x(t) + W*s(t-1))\n",
    "o(t)=softmax(V*s(t))\n",
    "\n",
    "x:输入数据 8000\n",
    "U 100×8000\n",
    "s:隐藏层数据 100\n",
    "W 100×100\n",
    "o:输出层数据 8000\n",
    "V 8000×100\n",
    "\n",
    "下面是对这个网络的参数的设置:\n",
    "词汇表的大小为(vocabulary size):C=8000\n",
    "隐藏层的大小为(hidden layer size):H=100\n",
    "ps:隐藏层的大小可以看作是我们网络的记忆力，它越大我们就可以学习更加复杂的部分，但是同时增加了计算量。\n",
    "\n",
    "训练参数量为2HC+H^2, 1,610,000\n",
    "\n",
    "模型的瓶颈为：\n",
    "由于输入的x中只有一个为1其他为0,所以仅仅选择U的一列，而不必进行全部乘法计算，所以，最大的乘法计算为V，所以希望词汇表尽可能的小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN的初始化方法很特别，原因待定。\n",
    "word_dim是词汇表的大小\n",
    "hidden_dim是隐藏层的大小\n",
    "现在先不考虑bptt_truncate参数，之后在BPTT中会讲解\n",
    "\n",
    "正向传播:\n",
    "现在使用上面的公式所定义的正向传播(预测词语的概率)\n",
    "在正向传播中我们将所有的隐藏层保存在s中\n",
    "我们在初始的隐藏层增加了一个额外的元素为0。\n",
    "不仅返回输出还返回隐藏状态\n",
    "o为单词的概率向量\n",
    "\n",
    "计算损失函数:\n",
    "我们可以先不用损失函数直接随机来生成，检验代码是否正确，然后，我们开始训练，当然首先的是损失函数的计算了。\n",
    "\n",
    "首先，我们通常使用交叉熵来计算损失，如果我们是有N个训练样本(我们文本中的词语)和C个类别(词汇表的大小)，那么我们计算的输出o和实际的标签y之间的误差为:\n",
    "L(y,o) = - 1/N×叠加(y_n*log(o_n))\n",
    "\n",
    "这个公式有点复杂，但是它的功能只是对我们的训练实例进行总结，然后根据我们的预测值的偏差来增加损失，我们使用了calculate_loss函数\n",
    "\n",
    "然后，我们使用了随机梯度下降算法SGD，其中，我们需要计算代价函数对于训练参数的偏导，由于RNN结构的特殊性，我们需要使用BackPropagation Through Time(BPTT)算法。\n",
    "\n",
    "因为网络中的每一步的参数都共享，每一步的输出的梯度不仅仅取决于当前时刻的计算，也受到之前时刻的影响，\n",
    "\n",
    "\n",
    "也就是应用了链式法则，我们使用了bptt函数，输入x,y，输出梯度。\n",
    "\n",
    "梯度检查(这部分略过):\n",
    "在执行反响传播的时候，最后执行梯度检查，这是一种验证实现正确的方法，梯度检查的思想是参数的导数等于该点的斜率，我们可以通过稍微改变参数然后除以变量来近似。\n",
    "\n",
    "SGD实施:\n",
    "使用之前的BPTT算法来计算梯度，实现SGD可以分为两步:\n",
    "1.函数sdg_step用于计算渐变并执行一批更新\n",
    "2.循环遍历训练及并调整学习率的外循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z)/((np.exp(z)).sum())\n",
    "\n",
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # 正向传递步骤数\n",
    "        T = len(x)\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # 输出结果保存\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # 对于每一步的运算\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self,x):\n",
    "        o,s = self.forward_propagation(x)\n",
    "        return np.argmax(o,axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        # x 为输入的特征，y 为实际的标签\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "\n",
    "\n",
    "    # Performs one step of SGD.\n",
    "    def numpy_sdg_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "\n",
    "    # Outer SGD Loop\n",
    "    # - model: The RNN model instance\n",
    "    # - X_train: The training data set\n",
    "    # - y_train: The training data labels\n",
    "    # - learning_rate: Initial learning rate for SGD\n",
    "    # - nepoch: Number of times to iterate through the complete dataset\n",
    "    # - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "    # nepoch 随机梯度下降的批数\n",
    "    # evaluate_loss_after 每隔多少批次进行误差的评估\n",
    "    def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        # We keep track of the losses so we can plot them later\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5  \n",
    "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(y_train)):\n",
    "                # One SGD step\n",
    "                model.numpy_sdg_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是用来看输入训练数据之后RNN的输出，没有训练，随机参数，下面的代码可以跳过不运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 72, 63, 13, 124, 5, 26, 1128, 208, 5, 324, 3, 329, 4, 112, 32, 75, 7, 4746, 4, 8, 84, 52, 9, 7, 3155, 1021, 492, 7534, 8, 133, 48, 3096, 4, 10, 95, 51, 4, 128, 17, 37, 314, 577, 2, 40]\n",
      "(45, 8000)\n",
      "[[0.00012408 0.0001244  0.00012603 ... 0.00012515 0.00012488 0.00012508]\n",
      " [0.00012536 0.00012582 0.00012436 ... 0.00012482 0.00012456 0.00012451]\n",
      " [0.00012387 0.0001252  0.00012474 ... 0.00012559 0.00012588 0.00012551]\n",
      " ...\n",
      " [0.00012471 0.0001243  0.00012524 ... 0.00012475 0.00012522 0.00012623]\n",
      " [0.00012564 0.00012431 0.00012481 ... 0.0001244  0.00012609 0.00012486]\n",
      " [0.00012447 0.00012509 0.00012469 ... 0.00012473 0.00012506 0.00012641]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o,s = model.forward_propagation(x_train[10])\n",
    "print(x_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是显示输入训练数据后，输出的字，没有训练，随机参数，可以跳过不运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 1874  224 6601 7299 6722 6892 3198\n",
      " 4480 5853 2926  261 4073 2371 6299 5376 4146 3761 7051 5981 1549 3765\n",
      " 4958 1835 6166 5192 2579 5879 4864 5132 6569 2800 2752 6821 4437 7021\n",
      " 3943 6912 3922]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是显示初始之后的误差，和理论计算的误差之间只有较小的差距，所以完成，可以跳过不运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eglym/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 8.987393\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(x_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 ms ± 3.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.numpy_sdg_step(x_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eglym/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-20 20:27:39: Loss after num_examples_seen=0 epoch=0: 8.987280\n",
      "2020-10-20 20:27:52: Loss after num_examples_seen=100 epoch=1: 8.976046\n",
      "2020-10-20 20:28:06: Loss after num_examples_seen=200 epoch=2: 8.959871\n",
      "2020-10-20 20:28:18: Loss after num_examples_seen=300 epoch=3: 8.929739\n",
      "2020-10-20 20:28:31: Loss after num_examples_seen=400 epoch=4: 8.851977\n",
      "2020-10-20 20:28:44: Loss after num_examples_seen=500 epoch=5: 6.804030\n",
      "2020-10-20 20:28:58: Loss after num_examples_seen=600 epoch=6: 6.271116\n",
      "2020-10-20 20:29:10: Loss after num_examples_seen=700 epoch=7: 6.004432\n",
      "2020-10-20 20:29:23: Loss after num_examples_seen=800 epoch=8: 5.833906\n",
      "2020-10-20 20:29:36: Loss after num_examples_seen=900 epoch=9: 5.715240\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = model.train_with_sgd(x_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
